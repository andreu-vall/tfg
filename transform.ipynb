{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will override the previous pkl\n",
      "[2024-05-13 08:54:05.297509]: Loading csv...\n",
      "[2024-05-13 08:54:06.754545]: Cutting text\n",
      "[2024-05-13 08:54:14.572543]: lost 83.15% of words\n",
      "[2024-05-13 08:54:14.572543]: Creating entities\n",
      "[2024-05-13 08:54:16.038539]: Keeping only most frequent words\n",
      "[2024-05-13 08:54:16.096514]: lost 2.22% of words\n",
      "[2024-05-13 08:54:16.096514]: Transforming data\n",
      "[2024-05-13 08:54:20.861509]: Data ready\n",
      "created, saving it\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "from utils.andreu import MyDataset\n",
    "\n",
    "data_path = f'data/amz-beauty-review' # right now, lower case reviewText\n",
    "text_max_words = 15\n",
    "vocab_size = 20_000\n",
    "\n",
    "amz_beauty_review = MyDataset.create_override(data_path, text_max_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will override the previous pkl\n",
      "[2024-05-13 08:54:59.189996]: Loading csv...\n",
      "[2024-05-13 08:55:00.804992]: Cutting text\n",
      "[2024-05-13 08:55:09.144992]: lost 83.86% of words\n",
      "[2024-05-13 08:55:09.144992]: Creating entities\n",
      "[2024-05-13 08:55:10.664000]: Keeping only most frequent words\n",
      "[2024-05-13 08:55:10.760995]: lost 3.67% of words\n",
      "[2024-05-13 08:55:10.760995]: Transforming data\n",
      "[2024-05-13 08:55:16.182990]: Data ready\n",
      "created, saving it\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "from utils.andreu import MyDataset\n",
    "\n",
    "data_path = f'data/amz-beauty'  # right now, like P5 summary + reviewText, capitalized\n",
    "text_max_words = 15             # sembla que tampoc es perdia tant capitalitzant\n",
    "vocab_size = 20_000\n",
    "\n",
    "amz_beauty = MyDataset.create_override(data_path, text_max_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-05-13 09:31:00.412896]: Loading csv...\n",
      "[2024-05-13 09:31:00.799885]: Cutting text\n",
      "[2024-05-13 09:31:02.460907]: lost 0.45% of words\n",
      "[2024-05-13 09:31:02.460907]: Creating entities\n",
      "[2024-05-13 09:31:03.094919]: Keeping only most frequent words\n",
      "[2024-05-13 09:31:03.147885]: lost 2.53% of words\n",
      "[2024-05-13 09:31:03.148890]: Transforming data\n",
      "[2024-05-13 09:31:08.137886]: Data ready\n",
      "created, saving it\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "from utils.andreu import MyDataset\n",
    "\n",
    "data_path = f'data/amz-beauty-summary'\n",
    "text_max_words = 15\n",
    "vocab_size = 20_000\n",
    "\n",
    "amz_beauty_summary = MyDataset.create_override(data_path, text_max_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(198502, 198502, 198502)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amz_beauty_review), len(amz_beauty), len(amz_beauty_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "will override the previous pkl\n",
      "[2024-05-13 08:56:24.792051]: Loading csv...\n",
      "[2024-05-13 08:56:25.231085]: Cutting text\n",
      "[2024-05-13 08:56:27.674049]: lost 12.04% of words\n",
      "[2024-05-13 08:56:27.674049]: Creating entities\n",
      "[2024-05-13 08:56:28.851079]: Keeping only most frequent words\n",
      "[2024-05-13 08:56:28.896079]: lost 0.18% of words\n",
      "[2024-05-13 08:56:28.896079]: Transforming data\n",
      "[2024-05-13 08:56:33.005044]: Data ready\n",
      "created, saving it\n",
      "saved\n"
     ]
    }
   ],
   "source": [
    "from utils.andreu import MyDataset\n",
    "\n",
    "data_path = f'data/amz-clothing' # right now, PETER processed quite a lot, entirely from Sentires toolkit\n",
    "text_max_words = 15\n",
    "vocab_size = 20_000\n",
    "\n",
    "amz_clothing = MyDataset.create_override(data_path, text_max_words, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179223"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(amz_clothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a review with some symbols\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_all_symbols(text):\n",
    "  \"\"\"Removes all symbols from a string using regular expressions.\n",
    "\n",
    "  Args:\n",
    "      text: The string to process.\n",
    "\n",
    "  Returns:\n",
    "      A new string with all symbols removed.\n",
    "  \"\"\"\n",
    "  # Compile a regular expression that matches any character that is not alphanumeric or whitespace\n",
    "  pattern = r\"[^\\w\\s]\"\n",
    "  return re.sub(pattern, \"\", text)\n",
    "\n",
    "# Example usage\n",
    "text = \"This is a!! review with some symbols!@#$%^&*\"\n",
    "text_without_symbols = remove_all_symbols(text)\n",
    "print(text_without_symbols)  # Output: This is a review with some symbols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
